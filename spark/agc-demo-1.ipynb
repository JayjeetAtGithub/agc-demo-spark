{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a6635a",
   "metadata": {},
   "source": [
    "# Lakehousing of HEP data using Apache Iceberg\n",
    "\n",
    "Jayjeet Chakraborty, University of California, Santa Cruz\n",
    "\n",
    "\n",
    "## Features of Iceberg:\n",
    "\n",
    "* Supports transactions\n",
    "* Hidden Partitioning\n",
    "* Schema Evolution\n",
    "* Time Travel and Rollbacks\n",
    "* Expressive SQL\n",
    "* Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215dcdca",
   "metadata": {},
   "source": [
    "## Adding Iceberg to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d672d2a",
   "metadata": {},
   "source": [
    "The Iceberg JAR file has to be copied to the Spark installation's JAR directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7cc88993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iceberg-spark-runtime-3.3_2.12-1.1.0.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/spark/jars | grep \"iceberg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31231497",
   "metadata": {},
   "source": [
    "We need to add some configurations options for Spark to pickup Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "829d0def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.extensions                   org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\r\n",
      "spark.sql.catalog.demo                 org.apache.iceberg.spark.SparkCatalog\r\n",
      "spark.sql.catalog.demo.warehouse       warehouse\r\n",
      "spark.sql.catalog.demo.type            hadoop\r\n",
      "spark.sql.defaultCatalog               demo\r\n",
      "spark.eventLog.enabled                 true\r\n",
      "spark.eventLog.dir                     /home/iceberg/spark-events\r\n",
      "spark.history.fs.logDirectory          /home/iceberg/spark-events\r\n",
      "spark.sql.catalogImplementation        in-memory\r\n"
     ]
    }
   ],
   "source": [
    "!cat /opt/spark/conf/spark-defaults.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9767e220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://2d4187795fe8:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f89a246de80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c20a9",
   "metadata": {},
   "source": [
    "## Creating an Iceberg table from a Parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1a8e6",
   "metadata": {},
   "source": [
    "We read NanoEvents data out of our Parquet file into a Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b23d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f131bde",
   "metadata": {},
   "source": [
    "We now create an Iceberg table out of this dataframe partitioned by the `event` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d440222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS hep;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531de16",
   "metadata": {},
   "source": [
    "For the purpose of the demo, we just use a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ad8360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\r\n"
     ]
    }
   ],
   "source": [
    "!ls dataset/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f405a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query1.sql  query3.sql\tquery5.sql    query6-2.sql  query8.sql\r\n",
      "query2.sql  query4.sql\tquery6-1.sql  query7.sql\r\n"
     ]
    }
   ],
   "source": [
    "!ls spark/queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deed4074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[run: int, luminosityBlock: bigint, event: bigint, MET: struct<pt:float,phi:float,sumet:float,significance:float,CovXX:float,CovXY:float,CovYY:float>, HLT: struct<IsoMu24_eta2p1:boolean,IsoMu24:boolean,IsoMu17_eta2p1_LooseIsoPFTau20:boolean>, PV: struct<npvs:int,x:float,y:float,z:float>, Muon: array<struct<pt:float,eta:float,phi:float,mass:float,charge:int,pfRelIso03_all:float,pfRelIso04_all:float,tightId:boolean,softId:boolean,dxy:float,dxyErr:float,dz:float,dzErr:float,jetIdx:int,genPartIdx:int>>, Electron: array<struct<pt:float,eta:float,phi:float,mass:float,charge:int,pfRelIso03_all:float,dxy:float,dxyErr:float,dz:float,dzErr:float,cutBasedId:boolean,pfId:boolean,jetIdx:int,genPartIdx:int>>, Photon: array<struct<pt:float,eta:float,phi:float,mass:float,charge:int,pfRelIso03_all:float,jetIdx:int,genPartIdx:int>>, Jet: array<struct<pt:float,eta:float,phi:float,mass:float,puId:boolean,btag:float>>, Tau: array<struct<pt:float,eta:float,phi:float,mass:float,charge:int,decayMode:int,relIso_all:float,jetIdx:int,genPartIdx:int,idDecayMode:boolean,idIsoRaw:float,idIsoVLoose:boolean,idIsoLoose:boolean,idIsoMedium:boolean,idIsoTight:boolean,idAntiEleLoose:boolean,idAntiEleMedium:boolean,idAntiEleTight:boolean,idAntiMuLoose:boolean,idAntiMuMedium:boolean,idAntiMuTight:boolean>>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e80b53b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.writeTo(\"hep\").using(\"iceberg\").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec5ea4",
   "metadata": {},
   "source": [
    "## Data, Metadata, WAL, and Snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5c07c",
   "metadata": {},
   "source": [
    "Data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6d40762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>file_path</th>\n",
       "        <th>file_size_in_bytes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00000-89-3e24a524-389a-48f7-a19f-f095ee48ed38-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00001-90-c2b20351-6b7c-41a0-80ad-c22c79004fe4-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00002-91-0725c118-80dd-4e72-a2e8-2ad83bfa3d7f-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00003-92-7a40a502-86be-4901-9532-45c293571f98-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00004-93-97d5f294-1f90-41e7-b4d1-5d019a640a22-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00005-94-b8220b97-270c-4fd6-b507-b6ede9986767-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00006-95-5109e633-f8e0-4634-9080-c04cae3bbb71-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00007-96-8aad0b58-c49d-4863-aba0-1042a9f91c55-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00008-97-05c26e57-e5ee-4fc0-91dd-57a37e97f12e-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00009-98-bd08d38e-909a-4cce-91fb-b0b600b98afe-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00010-99-b8640e49-7b98-41c3-961e-8b7b53f55d55-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00011-100-b5d88081-f9a9-45e8-84ca-ea3538dba8a5-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00012-101-5470dae4-6a2c-490e-89c8-51fd640cff4c-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00013-102-c6f2f209-0102-455e-a263-957d85dc6de8-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00014-103-7b758bb8-57fc-4a1d-a4ca-b4d949108bed-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>warehouse/hep/data/00015-104-6ce1c5ce-6ecc-4c16-a596-8a208a13b0ac-00001.parquet</td>\n",
       "        <td>349541</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------------------------------------------------------------------------+--------------------+\n",
       "|                                                                       file_path | file_size_in_bytes |\n",
       "+---------------------------------------------------------------------------------+--------------------+\n",
       "|  warehouse/hep/data/00000-89-3e24a524-389a-48f7-a19f-f095ee48ed38-00001.parquet |             349541 |\n",
       "|  warehouse/hep/data/00001-90-c2b20351-6b7c-41a0-80ad-c22c79004fe4-00001.parquet |             349541 |\n",
       "|  warehouse/hep/data/00002-91-0725c118-80dd-4e72-a2e8-2ad83bfa3d7f-00001.parquet |             349541 |\n",
       "|  warehouse/hep/data/00003-92-7a40a502-86be-4901-9532-45c293571f98-00001.parquet |             349541 |\n",
       "|  warehouse/hep/data/00004-93-97d5f294-1f90-41e7-b4d1-5d019a640a22-00001.parquet |             349541 |\n",
       "|  warehouse/hep/data/00005-94-b8220b97-270c-4fd6-b507-b6ede9986767-00001.parquet |             349541 |\n",
       "|  warehouse/hep/data/00006-95-5109e633-f8e0-4634-9080-c04cae3bbb71-00001.parquet |             349541 |\n",
       "|  warehouse/hep/data/00007-96-8aad0b58-c49d-4863-aba0-1042a9f91c55-00001.parquet |             349541 |\n",
       "|  warehouse/hep/data/00008-97-05c26e57-e5ee-4fc0-91dd-57a37e97f12e-00001.parquet |             349541 |\n",
       "|  warehouse/hep/data/00009-98-bd08d38e-909a-4cce-91fb-b0b600b98afe-00001.parquet |             349541 |\n",
       "|  warehouse/hep/data/00010-99-b8640e49-7b98-41c3-961e-8b7b53f55d55-00001.parquet |             349541 |\n",
       "| warehouse/hep/data/00011-100-b5d88081-f9a9-45e8-84ca-ea3538dba8a5-00001.parquet |             349541 |\n",
       "| warehouse/hep/data/00012-101-5470dae4-6a2c-490e-89c8-51fd640cff4c-00001.parquet |             349541 |\n",
       "| warehouse/hep/data/00013-102-c6f2f209-0102-455e-a263-957d85dc6de8-00001.parquet |             349541 |\n",
       "| warehouse/hep/data/00014-103-7b758bb8-57fc-4a1d-a4ca-b4d949108bed-00001.parquet |             349541 |\n",
       "| warehouse/hep/data/00015-104-6ce1c5ce-6ecc-4c16-a596-8a208a13b0ac-00001.parquet |             349541 |\n",
       "+---------------------------------------------------------------------------------+--------------------+"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT file_path, file_size_in_bytes FROM demo.hep.all_data_files;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca543f1",
   "metadata": {},
   "source": [
    "Metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cf6741b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>content</th>\n",
       "        <th>path</th>\n",
       "        <th>length</th>\n",
       "        <th>partition_spec_id</th>\n",
       "        <th>added_snapshot_id</th>\n",
       "        <th>added_data_files_count</th>\n",
       "        <th>existing_data_files_count</th>\n",
       "        <th>deleted_data_files_count</th>\n",
       "        <th>added_delete_files_count</th>\n",
       "        <th>existing_delete_files_count</th>\n",
       "        <th>deleted_delete_files_count</th>\n",
       "        <th>partition_summaries</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>warehouse/hep/metadata/cfd14711-8f76-4d0c-8132-2db56453e278-m0.avro</td>\n",
       "        <td>13369</td>\n",
       "        <td>0</td>\n",
       "        <td>242691178791709845</td>\n",
       "        <td>16</td>\n",
       "        <td>0</td>\n",
       "        <td>0</td>\n",
       "        <td>0</td>\n",
       "        <td>0</td>\n",
       "        <td>0</td>\n",
       "        <td>[]</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------+---------------------------------------------------------------------+--------+-------------------+--------------------+------------------------+---------------------------+--------------------------+--------------------------+-----------------------------+----------------------------+---------------------+\n",
       "| content |                                                                path | length | partition_spec_id |  added_snapshot_id | added_data_files_count | existing_data_files_count | deleted_data_files_count | added_delete_files_count | existing_delete_files_count | deleted_delete_files_count | partition_summaries |\n",
       "+---------+---------------------------------------------------------------------+--------+-------------------+--------------------+------------------------+---------------------------+--------------------------+--------------------------+-----------------------------+----------------------------+---------------------+\n",
       "|       0 | warehouse/hep/metadata/cfd14711-8f76-4d0c-8132-2db56453e278-m0.avro |  13369 |                 0 | 242691178791709845 |                     16 |                         0 |                        0 |                        0 |                           0 |                          0 |                  [] |\n",
       "+---------+---------------------------------------------------------------------+--------+-------------------+--------------------+------------------------+---------------------------+--------------------------+--------------------------+-----------------------------+----------------------------+---------------------+"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM demo.hep.manifests;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98ad8f",
   "metadata": {},
   "source": [
    "Write-Ahead Log files that enables transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f54a01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>timestamp</th>\n",
       "        <th>file</th>\n",
       "        <th>latest_snapshot_id</th>\n",
       "        <th>latest_schema_id</th>\n",
       "        <th>latest_sequence_number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2023-02-22 01:08:25.685000</td>\n",
       "        <td>warehouse/hep/metadata/v1.metadata.json</td>\n",
       "        <td>242691178791709845</td>\n",
       "        <td>0</td>\n",
       "        <td>0</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------------------------+-----------------------------------------+--------------------+------------------+------------------------+\n",
       "|                  timestamp |                                    file | latest_snapshot_id | latest_schema_id | latest_sequence_number |\n",
       "+----------------------------+-----------------------------------------+--------------------+------------------+------------------------+\n",
       "| 2023-02-22 01:08:25.685000 | warehouse/hep/metadata/v1.metadata.json | 242691178791709845 |                0 |                      0 |\n",
       "+----------------------------+-----------------------------------------+--------------------+------------------+------------------------+"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * from demo.hep.metadata_log_entries;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664bd55",
   "metadata": {},
   "source": [
    "Snapshot files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81f5a7e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>snapshot_id</th>\n",
       "        <th>manifest_list</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>242691178791709845</td>\n",
       "        <td>warehouse/hep/metadata/snap-242691178791709845-1-cfd14711-8f76-4d0c-8132-2db56453e278.avro</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+--------------------+--------------------------------------------------------------------------------------------+\n",
       "|        snapshot_id |                                                                              manifest_list |\n",
       "+--------------------+--------------------------------------------------------------------------------------------+\n",
       "| 242691178791709845 | warehouse/hep/metadata/snap-242691178791709845-1-cfd14711-8f76-4d0c-8132-2db56453e278.avro |\n",
       "+--------------------+--------------------------------------------------------------------------------------------+"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT snapshot_id, manifest_list FROM demo.hep.snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8c16d",
   "metadata": {},
   "source": [
    "## Running ADL benchmark queries on Iceberg\n",
    "\n",
    "https://github.com/iris-hep/adl-benchmarks-index/\n",
    "\n",
    "https://arxiv.org/pdf/2104.12615.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331cf70",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "53611e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1:  0.157670259475708\n",
      "Query 2:  0.22817230224609375\n",
      "Query 3:  0.257601261138916\n",
      "Query 4:  0.21548938751220703\n",
      "Query 5:  0.5015773773193359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 6-1:  1.1532421112060547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 6-2:  1.092984676361084\n",
      "Query 7:  0.653714656829834\n",
      "23/02/22 02:01:48 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier\n",
      "23/02/22 02:01:48 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier\n",
      "Query 8:  0.6282968521118164\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for query_id in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6-1\", \"6-2\", \"7\", \"8\"]:\n",
    "    with open(f\"spark/queries/query{query_id}.sql\") as f:\n",
    "        query = f.read()\n",
    "        query = query.replace(\"{table}\", \"hep\")\n",
    "        s = time.time()\n",
    "        resp = spark.sql(query).collect()\n",
    "        e = time.time()\n",
    "        print(f\"Query {query_id}: \", e-s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
